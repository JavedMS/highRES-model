# snakemake --resources mem_mb=700000 --slurm --default-resources \
# slurm_account=ec85 runtime=4320 mem_mb_per_cpu=21000 cpus_per_task=4 \
# --use-conda --cluster-cancel scancel --keep-going -j unlimited

# region:
# snakemake --resources mem_mb=700000 --slurm --default-resources \
# slurm_account=ec85 runtime=30 mem_mb_per_cpu=1100 cpus_per_task=4 \
# --use-conda --cluster-cancel scancel --keep-going -j unlimited

# TODO: Default profile is Fox, HPC, profile
configfile: "config/cluster_system_os_config.yaml"


import itertools
import pathlib

import pandas as pd

# Inputs to be used as wildcards. make changes based on the requirement
inputyears = [
    #"1995",
    "2010"
]

spatials = [
    "grid",
    #"region"
]
cutoffs_solar = [0.09]
cutoffs_onwind = [0.15]
cutoffs_offwind = [0.20]

#values = ["High", "Low"]
#solars = ["High", "Low"]
#cutoff_values = {"solar": 0.09, "onwind": 0.15, "offwind": 0.20}
#cutoffs_wind = [0, 0.15]
#cutoffs_wind = [0.15]

# Use product to create all possible combinations of the values in the lists
scenarios_product = list(itertools.product(inputyears, spatials, cutoffs_solar,
                                             cutoffs_onwind, cutoffs_offwind))

# Create the DataFrame from the list of all combinations
column_names = ['years', 'spatials', 'cutoffs_solar', 'cutoffs_onwind', 'cutoffs_offwind']
scenarios = pd.DataFrame(scenarios_product, columns=column_names)

#generating scenarios: six columns with naturs, faunas, samis, neighs, nothing, nowwind
# scenarios = (
#     pd.DataFrame(itertools.product(values, repeat=3))
#     .rename(
#         columns={
#             0: "naturs",
#             1: "faunas",
#             2: "samis",
#         }
#     )
#     .assign(neighs="Low")
#     .T.assign(nothing="None", nowind="High")
#     .T.reset_index(drop=True)
# )


# concatinating the scenarios by adding years column [16*7 dataframe]
# concatlist = []
# for year in inputyears:
#     concatlist.append(scenarios.assign(years=year))
# scenarios = pd.concat(concatlist).reset_index(drop=True)

# # (16*2) * 8 rows
# concatlist = []
# for solar in solars:
#     concatlist.append(scenarios.assign(solars=solar))
# scenarios = pd.concat(concatlist).reset_index(drop=True)

# concatlist = []
# for spatial in spatials:
#     concatlist.append(scenarios.assign(spatials=spatial))
# scenarios = pd.concat(concatlist).reset_index(drop=True)

# #final scenario dataframe: 
# concatlist = []
# for cutoff_wind in cutoffs_wind:
#     concatlist.append(scenarios.assign(cutoffs_wind=cutoff_wind))
# scenarios = pd.concat(concatlist).reset_index(drop=True)

# to run everything, comment the next line out!
# scenarios = scenarios.query("neighs=='None' or neighs=='Extreme' or solars=='High'")

# drop pointless cutoff grid scenarios
#scenarios = scenarios.drop(
#    scenarios.query("spatials=='grid' and cutoffs_wind!=0.0").index     # limiting the cutoffs_wind list to only 0.15
#).reset_index()

# droplist = [6,7,2,0,28]
# scenarios = scenarios.drop(droplist,axis=0)

#naturs = scenarios.naturs.to_list()
#faunas = scenarios.faunas.to_list()
#samis = scenarios.samis.to_list()
#neighs = scenarios.neighs.to_list()
years = scenarios.years.to_list()
spatials = scenarios.spatials.to_list()
cutoffs_solar = scenarios.cutoffs_solar.to_list()
cutoffs_onwind = scenarios.cutoffs_onwind.to_list()
cutoffs_offwind = scenarios.cutoffs_offwind.to_list()

# in grams per kWh
co2target = round(0, 1)

# gamsfilesha256 = "d8e033699604d4474a1c9db59140749509f30383dee532c1e85165617620776b"
# gamsfilesha256 = "af4a97e52ff41b137a24fb517b03e0e8268d879b01c7aefd7cae3074467ebad1"
# gamsfilesha256 = "b8ff95b0bc41fa7bd18ff192eac69b54ead25d72a851656c0d9e8fe41ca50177"
# gamsfilesha256 = "9b1b86c582b18822fcf9c65045f9c68605e951957cd934cc1713f8cf00a5f375"
# gamsfilesha256 = "7c58219c0ab4320c806c0460ebba547c10838313c6a1f10f6e6640aab90a4838"
# gamsfilesha256 = "9cced6b80dad705f6963e4dee1c48af4b8170d42444bb269b049ea8627d6c0e5"
# gamsfilesha256 = "686cc7b751384f1f0401289c1da84f8454221b7eb146bf1b65eb5dae2d08c22a"
# gamsfilesha256 = "21a42507bb4c2b9be551a4f35012d1e6645ea3ab401feaee7467c1af88b25c00"
gamsfilesha256 = "c2664b9bbf178e43b36260c429b21e6eeec6a0f8e8f7c9b6a24b504a4bc8ce25"
#gamsfilesha256 = "8760c1dfea64d41adcc75c16a4e0902f076e9f3a4468e455ad1d7fc1094a8a1b"

CORRECTOR_YEAR = 2019
num_near_wind_parks = 10
num_parallel_processes = 64

# Absolute path to your GAMS installation
gamspath = config["paths"]["gams"]

# Absolute path to the input data that is the same across all versions
shared_input_path = pathlib.Path(config["paths"]["shared_input"])

# Relative path the model code that is the same across all versions
# shared_code_path = "resources/4_model_code_shared/"
abs_shared_code_path = config["paths"]["abs_shared_code"]
# abs_shared_code_path = workflow.source_path("../resources/4_model_code_shared/")
# TODO: Get this to work, maybe with Pathlib

# Absolute path to the con directory:
abs_con_path = config["paths"]["abs_con"]

# Relative path to the geodata files that differentiate the scenarios
scenario_exclusions_path = "resources/scenario_exclusions/"

# Relative path to the results
# TODO: Vetle: --directory, flag for å sette results directory?
# resultspath = "results/"
resultspath = pathlib.Path(config["paths"]["resultsdir"])
pathlib.Path(resultspath).mkdir(parents=True, exist_ok=True)
# "/cluster/work/projects/ec85/joint-wind/model-aggregated/resultsoff/"

# Write scenarios to file, so results analysis script can check if they are all
# available.
scenarios.to_csv(resultspath / "scenarios.csv", sep="\t")

# Relative path to the built model and built inputs
yearonlypart = "models/{year}/"
#scenariopart = "{natur}_{fauna}_{sami}_{neigh}_{solar}_{spatial}_{cutoff_wind}/"
scenariopart = "{year}_{spatial}_{cutoff_solar}_{cutoff_onwind}_{cutoff_offwind}/"
modelpathyearonly = resultspath / yearonlypart
modelpath = modelpathyearonly / scenariopart
allyearpath = resultspath / "models/allyears/"
logpath = yearonlypart + scenariopart


localrules:
    all,
    build_gams,
    build_cplex_opt,
    build_technoeconomic_inputs,
    ensure_gams_template,
    rename_demand_file,
    build_regions_file,
    build_zones_file,
    build_hydro_areas,
    build_weather,
    build_vre_areas_file,
    build_vre_file,
    build_vre_gdx,
    build_hydro_capfac,
    build_hydrores_inflow,
    link_hydrores_inflow,
    convert_results,
    bias_correct_wind,      #not in EU version
    extract_wind_cap2area,  #not in EU version
    build_cutoff,           #not in EU version
    build_vre_parquet,
    build_vre_csv,
    compress_vre_gdx,
    compress_hydrores_inflow,
    convert_results_db_parquet, #not in EU version


rule all:
    input:
        expand(
            modelpath / "results.db.compressed",
            zip,
            year=years,     # these are all wildcards of snakefile
            #natur=naturs,
            #fauna=faunas,
            #sami=samis,
            #neigh=neighs,
            #solar=solars,
            spatial=spatials,
            cutoff_solar=cutoffs_solar, 
            cutoff_onwind=cutoffs_onwind,
            cutoff_offwind=cutoffs_offwind,
        ),


rule extract_wind_cap2area:
# TODO think about whether we really only should read from the high version
    # Reading only cap2area parameter from excel file & storing as windcap2area
    input:
        odsdatabase="resources/highres_gb_ext_database_High.ods",
        #odsdatabase="resources/highres_gb_ext_database_Low.ods",
    output:
        windcap2area=resultspath / "windcap2area",
    conda:
        "envs/build_windcap2area.yml"
    script:
        "scripts/build_windcap2area.py"


rule ensure_gams_template:
    input:
        "resources/highresraw.gms",
    output:
        gamsfile=ensure(resultspath / "highres.gms", sha256=gamsfilesha256),  
    run:
        import shutil
        
        shutil.copy2(input[0], output[0])


rule build_gams:
    input:
        rules.ensure_gams_template.output.gamsfile,
    params:
        co2intensity=co2target,
        sharedcodepath=abs_shared_code_path,
    output:
        modelpath / "highres.gms",
    script:
        "scripts/build_gams.py"


rule build_cplex_opt:
    input:
        "resources/cplex.opt",
        #ancient("resources/cplex.opt"), # TODO: What does this line do?
    output:
        modelpath / "cplex.opt",
    run:
        import shutil
        
        shutil.copy2(input[0], output[0])


rule build_zones_file:
    input:
        "resources/zones.csv",
        "workflow/scripts/build_zones.py",  #input/output paths always relative to the terminal
    output:
        modelpath / "zones.dd",
    script:
        "scripts/build_zones.py"        #script paths relative to the snakefile path

# where to make changes highres_gb_ext_database_Low for costs...?
# storage_costs: Li-ion battery, Storage+IDC stands for?,
# generator_costs: chnages from EU version --> solar, wind onshore, wind offshore, hydroROR, HydroRes and other techs as well.
# gen: windoffshore is not added. is it excluded from analysis? Added by me in highres_gb_ext_database_Low.ods 
# transmission allowed: distance and cap btw fylkes 
# sheets where changes to be made regarding costs: generator_costs (in capex of realted year & technology)...
# storage_costs: column_N (Storage (£k/MWh)), transmission_costs: (future transmission installation could be contrained based on scenarios)
# Transmission costs changing need to check further
# Import as technology used in excel file: its costs/capacities in gen, gen_lim_z, and gen_exist_z are unclear...
# gen_lim_z: sheet where fixed/upper limit of technologies defined. Windoffshore is fixed to zero currently?? windonshore and solar are not limited
# gen_exist_z: solar existing capacity is not added currently in technology and need to be added
# existing capacities of windoffshore, windonshore, and import are there in excel file

rule build_technoeconomic_inputs:
    input:
        "resources/zones.csv",          # zones need to change to get related region values from excel files
        "resources/gb_ext_scenarios.xls",
        #"resources/highres_gb_ext_database_{solar}.ods",    #solar wild card used here
        #"resources/highres_gb_ext_database_Low.ods",
        "resources/highres_gb_ext_database_High.ods",
        europecountriescsvlocation="resources/europe_countries.csv",
        europedemandcsvlocation="resources/europe_demand_2006-2015.csv", #nonrescale demand from 1951
        data2dd="workflow/scripts/data2dd_funcs.py",
    conda:
        "envs/build_technoeconomic.yml",
    output:
        modelpath / "{year}_temporal.dd",       # generating hourly temporal resolution
        modelpath / "BASE_co2_budget.dd",       # non_used file
        modelpath / "BASE_gen.dd",
        modelpath / "BASE_store.dd",
        modelpath / "trans.dd",
        demandfile=temp(modelpath / "BASE_norescale_demand_{year}.dd"), #only demand of wildcard year
    script:
        "scripts/gb_ext_data2dd.py"


rule rename_demand_file:
    input:
        rules.build_technoeconomic_inputs.output.demandfile,
    output:
        modelpath / "BASE_demand_{year}.dd",
    run:
        import shutil

        shutil.copy2(input[0], output[0])


rule build_hydro_areas:
    input:
        "resources/zones.csv",
    output:
        areashydro=temp(allyearpath / "available_area.txt"),
    script:
        "scripts/build_hydro_areas.py"

# CF corrected only for onshore data at grid cell level?
# params impact in corrected CF i.e., num_near_wind_parks?
# scripts versus notebooks?
rule bias_correct_wind:
    input:
        era5_corrector=abs_con_path + f"europe-era5_{CORRECTOR_YEAR}.nc",
        #era5_corrector=f"resources/bias/europe-era5_{CORRECTOR_YEAR}.nc",
        era5=abs_con_path + "europe-era5_{year}.nc",
        #era5="resources/bias/europe-era5_{year}.nc",
        wind_parks="resources/bias/nve_wind_parks.geojson",
        production="resources/bias/TimeSeries_UTC_kWh_2022_all.csv",
        europeshape="resources/bias/Onshore_EU-NUTS0_NO-NUTS3_UK-NG.geojson",
    output:
        uncorrected=modelpathyearonly / "uncorrected_wind_capacity_factors_{year}.nc",
        corrected=modelpathyearonly / "corrected_wind_capacity_factors_{year}.nc",
        corrected_simple=modelpathyearonly
        / "corrected_simple_wind_capacity_factors_{year}.nc",
        corrected_for_highres=modelpathyearonly
        / "corrected_wind_capacity_factors_{year}.parquet",
    params:
        corrector_year=CORRECTOR_YEAR,
        num_near_wind_parks=num_near_wind_parks,
        num_parallel_processes=num_parallel_processes,
    resources:
        nodes=num_parallel_processes,       #will be ignored other than cluster plateforms
    conda:
        "envs/environment.yaml",
    script:
        "scripts/bias_correct_wind.py"

# excluding all those grid cells having CF less than cut_off
rule build_cutoff:
    input:
        changedbiaswind=rules.bias_correct_wind.output.corrected_for_highres,
    output:
        exclusiontiff=modelpathyearonly / "cf_exclusion_{cutoff_onwind}.tif",
        #exclusiontiff=modelpathyearonly / "cf_exclusion_{cutoff_wind}.tif",
        #exclusiontiff=modelpathyearonly / "cf_exclusion_{cutoff_onwind}.tif",
        
    conda:
        "envs/implement_cutoff.yml",
#    params:
#        cutoff_onwind=cutoff_onwind,    
    notebook:
        "notebooks/build_cutoff_tiff.py.ipynb"


# this ensures that if cut_off is zero, no need to run the above rule
def cutoff_needed(wildcards):
#def cutoff_needed():
    returndict = {}
    if wildcards.cutoff_onwind != str(0.0):
    #if cutoff_values["onwind"] != 0.0:
        returndict = {"cutoffexclusiontiff": rules.build_cutoff.output.exclusiontiff}
    return returndict

# can be removed
# def myfunc(wildcards):
#     returndict = {}
#     if wildcards.natur != "None" and wildcards.natur != "Extreme":
#         returndict[
#             "naturex"
#         ] = scenario_exclusions_path + "Natur_{wildcards.natur}_Exc.zip".format(
#             wildcards=wildcards       # if the condition is met, returning the file path, i.e., {"faunaex": "/path/to/exclusions/Fauna_high_Exc.zip"}
#         )
#     if wildcards.neigh != "None" and wildcards.neigh != "Extreme":
#         returndict[
#             "neighex"
#         ] = scenario_exclusions_path + "Neigh_{wildcards.neigh}_Exc.zip".format(
#             wildcards=wildcards
#         )
#     return returndict

# Further base exclusions I can add are:
# Tech_All_Exc.zip: related to windonshore only: roads/railway 200m buffer, Airports 2km, watercourses/waterbodies/glaciers exclude
# Neigh_Low_Exc.zip: related to windonshore only: Buildings buffer 400m, visibility from populated areas 1Km buffer, ski/biking/walking routes 20m buffer
# Natur_High_Exc.zip: contains unqiue seven IUCN [0-6] numbers, categorizing the protected areas. 0: none IUCN classified areas...

rule build_weather:
    input:
        unpack(cutoff_needed),
        #unpack(myfunc),
        WDPA1a=shared_input_path / "geodata/onshore/WDPA_Ia_100.tiff",
        WDPA1b=shared_input_path / "geodata/onshore/WDPA_Ib_100.tiff",
        WDPA2=shared_input_path / "geodata/onshore/WDPA_II_100.tiff",
        WDPA3=shared_input_path / "geodata/onshore/WDPA_III_100.tiff",
        WDPA4=shared_input_path / "geodata/onshore/WDPA_IV_100.tiff",
        elevation=shared_input_path / "geodata/onshore/2000m.shp.zip",
        slope=shared_input_path / "geodata/onshore/15degrees.shp.zip",
        euroshape=shared_input_path / "geodata/onshore/shapes/NO-NUTS3.geojson",
        eurooffshoreshape=shared_input_path
        / ("geodata/offshore/BOTTOM_MOUNTED_EUROPE_NUTS0" "_NORWAY_NUTS3.geojson"),
        weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
        corine=shared_input_path / "geodata/onshore/corine.tif",
        #cap2area="resources/highres_gb_ext_database_{solar}.ods",
        #cap2area=rules.extract_wind_cap2area.output.windcap2area,
        cap2area=ancient("resources/windcap2area"),         #it is assumed available already
        wind_parks="resources/bias/nve_wind_parks.geojson",
        pvlow=scenario_exclusions_path + "PV_Low_Exc.zip",
        windlow=scenario_exclusions_path + "co.geotiff",
        tech=scenario_exclusions_path + "Techn_All_Exc.zip",
        neighex=scenario_exclusions_path + "Neigh_Low_Exc.zip",     #added as base exclusion for me, is it ok?
        #faunaex=scenario_exclusions_path + "Fauna_{fauna}_Exc.zip",
        #naturex=scenario_exclusions_path + "Natur_{natur}_Exc.zip",
        #neighex=scenario_exclusions_path + "Neigh_{neigh}_Exc.zip",
        #samiex=scenario_exclusions_path + "Sami_{sami}_Exc.zip",
        changedbiaswind=rules.bias_correct_wind.output.corrected_for_highres,
        #cutoffexclusiontiff=rules.build_cutoff.output.exclusiontiff
    output:
        cf_exclusion_solar=temp(modelpath / "cf_exclusion_solar.tif"),  #exclusion only based on CF cutoff
        cf_exclusion_windon=temp(modelpath / "cf_exclusion_windon.tif"),
        cf_exclusion_windoff=temp(modelpath / "cf_exclusion_windoff.tif"),
        indreg=temp(modelpath / "indices_region.csv"),
        areassolar=temp(modelpath / "areas_norway_grid_solar.csv"),
        areaswindonshore=temp(modelpath / "areas_norway_grid_wind_onshore.csv"),
        areaswindoffshore=temp(modelpath / "areas_norway_grid_wind_offshore.csv"),
        changedbiaswindcsv=temp(modelpath / "capacity-factors_wind_norway-g_{year}.csv"),
        capfacfile=temp(modelpath / "capacity-factors_solar_norway-g_{year}.csv"),
        # FIXME the files indreg and cf_exclusion_{solar,windon,windoff} are
        # only necessary under some conditions currently when those are not met,
        # we create an empty file
        # the cleaner solution might be to outsource the creation of that file
        # to an extra rule and have an input function conditional on the
        # wildcard spatial for that rule
    conda:
        "envs/build_weather.yml"
    log:
        notebook="logs/notebooks/" + logpath + "highRES-Norway_grid.py.ipynb",
    resources:
        mem_mb=50000,  # TODO: Find out how much this rule needs,
        # should be constant (regional/grid)
        nodes=12,
    params:
        sharedinputpath=shared_input_path,
        #cutoffs=cutoff_values,
    notebook:
        "notebooks/highRES-Norway_grid.py.ipynb"


rule build_hydro_capfac:
# these inputs are not used in notebook, rather directly retrieved using path
    input:
        eiahydrogen="resources/EIA_hydro_generation_1995_2000_2014.csv",
        vannkraft="resources/Vannkraftverk.csv",
        hydroinstalledcap="resources/hydro_installed_cap.tsv",
    output:
        hydrororcapfac=temp(
            modelpathyearonly / "capacity-factors_hydro_norway-3_{year}.csv"
        ),
        hydroresinfl=temp(modelpathyearonly / "inflow_hydro-res_norway-3_{year}.csv"),
    conda:
        "envs/build_hydro_capfac.yml"
    params:
        sharedinputpath=shared_input_path,
    notebook:
        "notebooks/highRES_norway3_hydro.py.ipynb"


def regionsonlyforgrid(wildcards):
    returndict = {}
    if wildcards.spatial == "grid":
        returndict["indreg"] = rules.build_weather.output.indreg
    return returndict


rule build_regions_file:
    input:
        "workflow/scripts/build_regions.py",
        unpack(regionsonlyforgrid),
        zonescsv="resources/zones.csv",
        #rules.build_weather.output.indreg,
    output:
        regionsdd=modelpath / "_regions.dd",
    script:
        "scripts/build_regions.py"


# def build_vre_areas_file_func(wildcards):
#    returndict = {}
#    if wildcards.neigh != "Extreme":
#        returndict['areaswindon'] = rules.build_weather.output.areaswindonshore
#    return returndict


rule build_vre_areas_file:
    input:
        areashydro=rules.build_hydro_areas.output.areashydro,
        # unpack(build_vre_areas_file_func),
        areassolar=rules.build_weather.output.areassolar,
        areaswindon=rules.build_weather.output.areaswindonshore,
        areaswindoff=rules.build_weather.output.areaswindoffshore,
        vreareaheader="resources/vre_areas_header.txt",
        genericfooter="resources/generic_footer.txt",
    output:
        modelpath / "vre_areas_{year}_.dd",     #main_file
        unsorted=temp(modelpath / "vre_areas_unsorted.txt"),
        areassorted=temp(modelpath / "vre_areas_sorted.txt"),
    script:
        "scripts/build_vre_areas_file.py"



rule build_hydrores_inflow:
    input:
        hydroresinfl=rules.build_hydro_capfac.output.hydroresinfl,
    output:
        inflowgdx=modelpathyearonly / "hydro_res_inflow_{year}.gdx",
    shell:
        """
        {gamspath} csv2gdx {input.hydroresinfl} output={output.inflowgdx} ID=hydro_inflow
        Index='(1,2,3)' Value='(4)' UseHeader=True StoreZero=True
        """
        #gamspath + (
        #    "csv2gdx {input} output={output} ID=hydro_inflow "
        #    "Index='(1,2,3)' Value='(4)' UseHeader=True StoreZero=True"
        #)


rule compress_hydrores_inflow:
    input:
        rules.build_hydrores_inflow.output.inflowgdx,
    output:
        compressdone=touch(modelpathyearonly / "hydrores_inflow_gdx.compressed"),
    shell:
        gamspath + "gdxcopy -V7C -Replace {input}"


rule link_hydrores_inflow:
    input:
        rules.compress_hydrores_inflow.output.compressdone,
        rules.build_hydrores_inflow.output.inflowgdx,
    output:
        inflowgdx=modelpath / "hydro_res_inflow_{year}.gdx",    #shifting hydro_res_inlfow from modelpathyearonly path to modelpath.
    shell:
        "ln -sr {input[1]} {output}"

# vre_file contains: windon, windoff, solar, and hydroror CF.
rule build_vre_file:
    input:
        cfwsng=rules.build_weather.output.capfacfile,       #contains solar and windoffshore CF
        cfhn3=rules.build_hydro_capfac.output.hydrororcapfac,
        changedbiaswindcsv=rules.build_weather.output.changedbiaswindcsv,   #contains corrected windonshore CF
    output:
        vrefile=temp(modelpath / "vre_{year}_.csv"),
    shell:
        (
            "cat {input[cfwsng]} {input[changedbiaswindcsv]} {input[cfhn3]} | sed 's/bottom//'"
            " > {output[vrefile]}"
        )


rule build_vre_parquet:
    input:
        rules.build_vre_file.output.vrefile,
    output:
        modelpath / "vre_{year}_.parquet",
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/build_vre_parquet.py"


rule build_vre_csv:
    input:
        modelpath / "vre_{year}_.parquet",
    output:
        csvgdx=temp(modelpath / "vre_{year}_tmp.csv"),
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/build_vre_csv.py"


rule build_vre_gdx:
    input:
        rules.build_vre_csv.output.csvgdx,
    output:
        bigvregdx=temp(modelpath / "vre_{year}_.gdx"),
    shell:
        gamspath + (
            "csv2gdx {input} output={output} ID=vre_gen Index='(1,2,3)'"
            " Value='(4)' UseHeader=True StoreZero=True"
        )


rule compress_vre_gdx:
    input:
        rules.build_vre_gdx.output.bigvregdx,
    output:
        touch(modelpath / "vre_gdx.compressed"),
    shell:
        gamspath + "gdxcopy -V7C -Replace {input}"


""" def inputfilelist(wildcards):
    returndict = {}
    returndict['regionsfile'] = modelpath / "_regions.dd"
    returndict['zonesfile'] = modelpath / "zones.dd"
    returndict['temporalfile'] = modelpath / "{year}_temporal.dd"
    returndict['vreareafile'] = modelpath / "vre_areas_{year}_.dd"
    returndict['demandfile'] = modelpath / "BASE_demand_{year}.dd"
    returndict['capfacfilecompressed'] = modelpath / "vre_gdx.compressed"
    returndict['capfacfile'] = modelpath / "vre_{year}_.gdx"
    returndict['hydroresinflowfile'] = modelpath / "hydro_res_inflow_{year}.gdx"
    returndict['co2budgetfile'] = modelpath / "BASE_co2_budget.dd"
    returndict['genparamsfile'] = modelpath / "BASE_gen.dd"
    returndict['storeparamsfile'] = modelpath / "BASE_store.dd"
    returndict['transparamsfile'] = modelpath / "trans.dd"
    return returndict """


rule build_inputs:
    input:
        modelpath / "highres.gms",
        modelpath / "cplex.opt",
        abs_shared_code_path + "highres_data_input.gms",
        abs_shared_code_path + "highres_hydro.gms",
        abs_shared_code_path + "highres_results.gms",
        abs_shared_code_path + "highres_storage_setup.gms",
        abs_shared_code_path + "highres_storage_uc_setup.gms",
        abs_shared_code_path + "highres_uc_setup.gms",
        modelpath / "_regions.dd",
        modelpath / "zones.dd",
        modelpath / "{year}_temporal.dd",
        modelpath / "vre_areas_{year}_.dd",
        modelpath / "BASE_demand_{year}.dd",
        modelpath / "vre_gdx.compressed",
        modelpath / "vre_{year}_.gdx",
        modelpath / "hydro_res_inflow_{year}.gdx",
        modelpath / "BASE_co2_budget.dd",
        modelpath / "BASE_gen.dd",
        modelpath / "BASE_store.dd",
        modelpath / "trans.dd",
    output:
        touch(modelpath / "inputs.finished"),


rule run_model:
    input:
        modelpath / "highres.gms",
        modelpath / "cplex.opt",
        abs_shared_code_path + "highres_data_input.gms",
        abs_shared_code_path + "highres_hydro.gms",
        abs_shared_code_path + "highres_results.gms",
        abs_shared_code_path + "highres_storage_setup.gms",
        abs_shared_code_path + "highres_storage_uc_setup.gms",
        abs_shared_code_path + "highres_uc_setup.gms",
        modelpath / "inputs.finished",
        modelpath / "vre_{year}_.gdx",
    params:
        gamspath=gamspath,
        modelpath=str(modelpath),
    # retries: 3
    log:
        str(modelpath) + "/highres.lst",
        str(modelpath) + "/highres.log",
    output:
        modelresults=protected(modelpath / "results.gdx"),
        modelresultsdd=protected(modelpath / "results.db"),
    script:
        "scripts/run_gams.sh"


rule convert_results:
    input:
        rules.run_model.output.modelresults,
    output:
        resultsdb=ensure(protected(modelpath / "results.db"), non_empty=True),
    shell:
        gamspath + "gdx2sqlite -i {input} -o {output} -fast"


rule convert_results_db_parquet:
    input:
        rules.run_model.output.modelresultsdd,
    output:
        compressdone=touch(modelpath / "results.db.compressed"),
    resources:
        mem_mb=50000,
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/convert_results_sqlite_parquet.py"


# rule plot_bias_correction:
#     input:
#         era5=f"data/input/europe-era5_{CORRECTOR_YEAR}.nc",
#         production="data/input/TimeSeries_UTC_kWh_2022_all.csv",
#         europeshape="data/input/Onshore_EU-NUTS0_NO-NUTS3_UK-NG.geojson",
#         wind_parks="data/input/nve_wind_parks.geojson",
#         uncorrected_cap_factors=f"data/output/uncorrected_wind_capacity_factors_{CORRECTOR_YEAR}.nc",
#         corrected_cap_factors=f"data/output/corrected_wind_capacity_factors_{CORRECTOR_YEAR}.nc",
#         corrected_cap_factors_simple=f"data/output/corrected_simple_wind_capacity_factors_{CORRECTOR_YEAR}.nc",
#     output:
#         directory("data/output/plots"),
#     params:
#         corrector_year=CORRECTOR_YEAR,
#     conda:
#         "envs/environment.yaml"
#     script:
#         "scripts/plot_bias_correction.py"
